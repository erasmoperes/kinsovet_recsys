{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a4513a-0b1d-435d-bc36-45e7bd70e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pocketbase import PocketBase\n",
    "import json\n",
    "\n",
    "POCKETBASE_URL = \"http://localhost:8090\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed99041a-d705-44f0-ac5e-9a180021a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = PocketBase(POCKETBASE_URL)\n",
    "\n",
    "similar_movies_raw = pb.collection(\"similar_movies\").get_full_list()\n",
    "similar_movies = {r.movie: set(r.recommendations) for r in similar_movies_raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14ea7fb-82a2-4488-926c-06851b4dc082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нод: 14276\n",
      "Рёбер: 35326\n",
      "Макс. рёбер: 101894950\n",
      "Плотность: 0.000347 (0.0347%)\n"
     ]
    }
   ],
   "source": [
    "# Строим граф\n",
    "graph = similar_movies\n",
    "\n",
    "# Симметризация: добавляем обратные связи\n",
    "for node, neighbors in list(graph.items()):\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor not in graph:\n",
    "            # print(f\"node: {node} | neighbor: {neighbor}\")\n",
    "            graph[neighbor] = set()\n",
    "        graph[neighbor].add(node)\n",
    "\n",
    "# Выбрасываем плохо заполненные ноды\n",
    "NODE_EDGES_THS = 0\n",
    "for node, neighbors in list(graph.items()):\n",
    "    if len(neighbors) < NODE_EDGES_THS:\n",
    "        del graph[node]\n",
    "\n",
    "# Считаем статистику\n",
    "num_nodes = len(graph)\n",
    "num_edges = sum(len(neighbors) for neighbors in graph.values()) // 2  # каждое ребро посчитано дважды\n",
    "max_edges = num_nodes * (num_nodes - 1) // 2\n",
    "density = num_edges / max_edges if max_edges > 0 else 0\n",
    "\n",
    "print(f\"Нод: {num_nodes}\")\n",
    "print(f\"Рёбер: {num_edges}\")\n",
    "print(f\"Макс. рёбер: {max_edges}\")\n",
    "print(f\"Плотность: {density:.6f} ({density*100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e7df487-eb50-4798-adec-320f7a0d0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего рёбер: 35326\n",
      "Train рёбер: 28261\n",
      "Test рёбер: 7065\n",
      "Доля test: 20.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def split_edges(graph, test_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Убираем test_fraction рёбер из графа.\n",
    "    Гарантируем: у каждой ноды остаётся минимум 1 ребро.\n",
    "    Возвращает (train_graph, test_edges).\n",
    "    \"\"\"\n",
    "    # Собираем все рёбра (каждое один раз)\n",
    "    all_edges = set()\n",
    "    for node, neighbors in graph.items():\n",
    "        for neighbor in neighbors:\n",
    "            edge = tuple(sorted((node, neighbor)))\n",
    "            all_edges.add(edge)\n",
    "    \n",
    "    all_edges = list(all_edges)\n",
    "    random.shuffle(all_edges)\n",
    "    \n",
    "    # Степени нод\n",
    "    degree = defaultdict(int)\n",
    "    for u, v in all_edges:\n",
    "        degree[u] += 1\n",
    "        degree[v] += 1\n",
    "    \n",
    "    # Текущие степени (будем уменьшать при удалении ребра)\n",
    "    current_degree = dict(degree)\n",
    "    \n",
    "    test_edges = []\n",
    "    train_edges = []\n",
    "    target_test = int(len(all_edges) * test_fraction)\n",
    "    \n",
    "    for u, v in all_edges:\n",
    "        # Можем убрать ребро, только если у обеих нод останется >= 1\n",
    "        if len(test_edges) < target_test and current_degree[u] > 1 and current_degree[v] > 1:\n",
    "            test_edges.append((u, v))\n",
    "            current_degree[u] -= 1\n",
    "            current_degree[v] -= 1\n",
    "        else:\n",
    "            train_edges.append((u, v))\n",
    "    \n",
    "    # Строим train граф\n",
    "    train_graph = defaultdict(set)\n",
    "    for u, v in train_edges:\n",
    "        train_graph[u].add(v)\n",
    "        train_graph[v].add(u)\n",
    "    \n",
    "    # Все ноды должны присутствовать (даже если изолированных нет по построению)\n",
    "    for node in graph:\n",
    "        if node not in train_graph:\n",
    "            train_graph[node] = set()\n",
    "    \n",
    "    print(f\"Всего рёбер: {len(all_edges)}\")\n",
    "    print(f\"Train рёбер: {len(train_edges)}\")\n",
    "    print(f\"Test рёбер: {len(test_edges)}\")\n",
    "    print(f\"Доля test: {len(test_edges)/len(all_edges):.2%}\")\n",
    "    \n",
    "    return dict(train_graph), test_edges\n",
    "\n",
    "train_graph, test_edges = split_edges(graph, test_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "n8cysuvk70g",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training node2vec on train graph...\n",
      "Edgelist: 28261 lines, first 3: ['75y6sp48613id9n\\toykxnpe1jf3c2di\\n', '75y6sp48613id9n\\tkrxr7vgpypqzx0v\\n', '0nv2e39y202ngl2\\tj3g6c1romcu2r71\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 13:14:24,701 : INFO : collecting all words and their counts\n",
      "2026-02-26 13:14:24,702 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2026-02-26 13:14:24,722 : INFO : PROGRESS: at sentence #10000, processed 210000 words, keeping 13910 word types\n",
      "2026-02-26 13:14:24,743 : INFO : PROGRESS: at sentence #20000, processed 420000 words, keeping 14236 word types\n",
      "2026-02-26 13:14:24,768 : INFO : PROGRESS: at sentence #30000, processed 630000 words, keeping 14272 word types\n",
      "2026-02-26 13:14:24,793 : INFO : PROGRESS: at sentence #40000, processed 840000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,816 : INFO : PROGRESS: at sentence #50000, processed 1050000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,842 : INFO : PROGRESS: at sentence #60000, processed 1260000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,867 : INFO : PROGRESS: at sentence #70000, processed 1470000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,897 : INFO : PROGRESS: at sentence #80000, processed 1680000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,926 : INFO : PROGRESS: at sentence #90000, processed 1890000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,953 : INFO : PROGRESS: at sentence #100000, processed 2100000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:24,977 : INFO : PROGRESS: at sentence #110000, processed 2310000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:25,004 : INFO : PROGRESS: at sentence #120000, processed 2520000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:25,034 : INFO : PROGRESS: at sentence #130000, processed 2730000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:25,065 : INFO : PROGRESS: at sentence #140000, processed 2940000 words, keeping 14276 word types\n",
      "2026-02-26 13:14:25,078 : INFO : collected 14276 word types from a corpus of 2997960 raw words and 142760 sentences\n",
      "2026-02-26 13:14:25,079 : INFO : Creating a fresh vocabulary\n",
      "2026-02-26 13:14:25,117 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=0 retains 14276 unique words (100.00% of original 14276, drops 0)', 'datetime': '2026-02-26T13:14:25.116965', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2026-02-26 13:14:25,118 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=0 leaves 2997960 word corpus (100.00% of original 2997960, drops 0)', 'datetime': '2026-02-26T13:14:25.118488', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2026-02-26 13:14:25,177 : INFO : deleting the raw counts dictionary of 14276 items\n",
      "2026-02-26 13:14:25,179 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2026-02-26 13:14:25,180 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 2997960 word corpus (100.0%% of prior 2997960)', 'datetime': '2026-02-26T13:14:25.180139', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2026-02-26 13:14:25,267 : INFO : estimated required memory for 14276 words and 128 dimensions: 21756624 bytes\n",
      "2026-02-26 13:14:25,268 : INFO : resetting layer weights\n",
      "2026-02-26 13:14:25,280 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2026-02-26T13:14:25.280837', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'build_vocab'}\n",
      "2026-02-26 13:14:25,282 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 14276 vocabulary and 128 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2026-02-26T13:14:25.282567', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2026-02-26 13:14:26,300 : INFO : EPOCH 0 - PROGRESS: at 12.67% examples, 375075 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:27,309 : INFO : EPOCH 0 - PROGRESS: at 23.01% examples, 341194 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:28,311 : INFO : EPOCH 0 - PROGRESS: at 37.34% examples, 370237 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:29,341 : INFO : EPOCH 0 - PROGRESS: at 51.01% examples, 377288 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:30,359 : INFO : EPOCH 0 - PROGRESS: at 65.02% examples, 384348 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:31,390 : INFO : EPOCH 0 - PROGRESS: at 79.69% examples, 391493 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:32,400 : INFO : EPOCH 0 - PROGRESS: at 95.69% examples, 403370 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:32,686 : INFO : EPOCH 0: training on 2997960 raw words (2997960 effective words) took 7.4s, 405208 effective words/s\n",
      "2026-02-26 13:14:33,706 : INFO : EPOCH 1 - PROGRESS: at 14.34% examples, 423713 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:34,752 : INFO : EPOCH 1 - PROGRESS: at 31.01% examples, 451155 words/s, in_qsize 8, out_qsize 0\n",
      "2026-02-26 13:14:35,793 : INFO : EPOCH 1 - PROGRESS: at 45.68% examples, 441536 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:36,795 : INFO : EPOCH 1 - PROGRESS: at 61.68% examples, 450618 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:37,828 : INFO : EPOCH 1 - PROGRESS: at 77.69% examples, 453448 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:38,848 : INFO : EPOCH 1 - PROGRESS: at 94.36% examples, 459485 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:39,205 : INFO : EPOCH 1: training on 2997960 raw words (2997960 effective words) took 6.5s, 460283 effective words/s\n",
      "2026-02-26 13:14:40,211 : INFO : EPOCH 2 - PROGRESS: at 14.67% examples, 439612 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:41,241 : INFO : EPOCH 2 - PROGRESS: at 30.01% examples, 443210 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:42,245 : INFO : EPOCH 2 - PROGRESS: at 45.35% examples, 448003 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:43,255 : INFO : EPOCH 2 - PROGRESS: at 60.68% examples, 449881 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:44,258 : INFO : EPOCH 2 - PROGRESS: at 76.02% examples, 451553 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:45,268 : INFO : EPOCH 2 - PROGRESS: at 90.36% examples, 447271 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:45,958 : INFO : EPOCH 2: training on 2997960 raw words (2997960 effective words) took 6.7s, 444370 effective words/s\n",
      "2026-02-26 13:14:47,020 : INFO : EPOCH 3 - PROGRESS: at 15.00% examples, 426607 words/s, in_qsize 6, out_qsize 1\n",
      "2026-02-26 13:14:48,028 : INFO : EPOCH 3 - PROGRESS: at 30.01% examples, 436034 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:49,047 : INFO : EPOCH 3 - PROGRESS: at 46.35% examples, 450879 words/s, in_qsize 8, out_qsize 0\n",
      "2026-02-26 13:14:50,088 : INFO : EPOCH 3 - PROGRESS: at 62.02% examples, 451008 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:51,132 : INFO : EPOCH 3 - PROGRESS: at 76.02% examples, 441094 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:52,170 : INFO : EPOCH 3 - PROGRESS: at 90.69% examples, 438182 words/s, in_qsize 8, out_qsize 0\n",
      "2026-02-26 13:14:52,753 : INFO : EPOCH 3: training on 2997960 raw words (2997960 effective words) took 6.8s, 441674 effective words/s\n",
      "2026-02-26 13:14:53,764 : INFO : EPOCH 4 - PROGRESS: at 14.67% examples, 437466 words/s, in_qsize 8, out_qsize 0\n",
      "2026-02-26 13:14:54,769 : INFO : EPOCH 4 - PROGRESS: at 30.68% examples, 457445 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:55,810 : INFO : EPOCH 4 - PROGRESS: at 45.68% examples, 448870 words/s, in_qsize 8, out_qsize 0\n",
      "2026-02-26 13:14:56,815 : INFO : EPOCH 4 - PROGRESS: at 62.02% examples, 458366 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:57,842 : INFO : EPOCH 4 - PROGRESS: at 76.35% examples, 450289 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:58,851 : INFO : EPOCH 4 - PROGRESS: at 92.36% examples, 454503 words/s, in_qsize 7, out_qsize 0\n",
      "2026-02-26 13:14:59,307 : INFO : EPOCH 4: training on 2997960 raw words (2997960 effective words) took 6.5s, 457808 effective words/s\n",
      "2026-02-26 13:14:59,308 : INFO : Word2Vec lifecycle event {'msg': 'training on 14989800 raw words (14989800 effective words) took 34.0s, 440549 effective words/s', 'datetime': '2026-02-26T13:14:59.308801', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2026-02-26 13:14:59,311 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=14276, vector_size=128, alpha=0.025>', 'datetime': '2026-02-26T13:14:59.311729', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 14276\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import logging\n",
    "from pecanpy.pecanpy import SparseOTF\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def graph_to_edgelist(g, path):\n",
    "    \"\"\"Записываем граф в формат edgelist для pecanpy.\"\"\"\n",
    "    with open(path, \"w\") as f:\n",
    "        written = set()\n",
    "        for node, neighbors in g.items():\n",
    "            for neighbor in neighbors:\n",
    "                edge = tuple(sorted((node, neighbor)))\n",
    "                if edge not in written:\n",
    "                    f.write(f\"{edge[0]}\\t{edge[1]}\\n\")\n",
    "                    written.add(edge)\n",
    "\n",
    "def train_node2vec(g, dimensions, window, walk_length, num_walks, p=1.0, q=1.0, workers=4):\n",
    "    \"\"\"Обучаем node2vec через pecanpy (SparseOTF) + gensim Word2Vec.\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".edgelist\", delete=False) as f:\n",
    "        edgelist_path = f.name\n",
    "    \n",
    "    try:\n",
    "        graph_to_edgelist(g, edgelist_path)\n",
    "        \n",
    "        # Проверяем файл\n",
    "        with open(edgelist_path) as f:\n",
    "            lines = f.readlines()\n",
    "            print(f\"Edgelist: {len(lines)} lines, first 3: {lines[:3]}\")\n",
    "        \n",
    "        pecanpy_graph = SparseOTF(p=p, q=q, workers=workers)\n",
    "        pecanpy_graph.read_edg(edgelist_path, weighted=False, directed=False)\n",
    "        \n",
    "        walks = pecanpy_graph.simulate_walks(num_walks=num_walks, walk_length=walk_length)\n",
    "        \n",
    "        # Word2Vec на прогулках\n",
    "        model = Word2Vec(\n",
    "            walks,\n",
    "            vector_size=dimensions,\n",
    "            window=window,\n",
    "            min_count=0,\n",
    "            sg=1,  # skip-gram\n",
    "            workers=workers,\n",
    "            epochs=5,\n",
    "        )\n",
    "        return model\n",
    "    finally:\n",
    "        os.unlink(edgelist_path)\n",
    "\n",
    "print(\"Training node2vec on train graph...\")\n",
    "model = train_node2vec(train_graph, dimensions=128, window=5, walk_length=20, num_walks=10, p=1, q=1)\n",
    "print(f\"Vocabulary size: {len(model.wv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3uxr6hfvw84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b126d2be4b4904ac60bb89dc7a340f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/6910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bucket      Count      R@5     R@10     R@20      MRR\n",
      "-----------------------------------------------------\n",
      "all          6910   0.1511   0.2685   0.3988   0.1244\n",
      "1-3           825   0.3091   0.4279   0.5139   0.1815\n",
      "3-6          2171   0.2353   0.3802   0.5100   0.1563\n",
      "6-10         1924   0.1074   0.2423   0.3982   0.1086\n",
      "10-15        1257   0.0463   0.1361   0.2822   0.0868\n",
      "15+           733   0.0186   0.0535   0.1411   0.0715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def evaluate(model, test_edges, graph, ks=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Для каждой ноды, у которой были убраны рёбра:\n",
    "    - Ранжируем все остальные ноды по cosine similarity эмбеддингов\n",
    "    - Считаем Recall@K и MRR\n",
    "    - Группируем по бакетам степени ноды (в исходном графе)\n",
    "    \"\"\"\n",
    "    # Собираем test-соседей для каждой ноды\n",
    "    test_neighbors = defaultdict(set)\n",
    "    for u, v in test_edges:\n",
    "        test_neighbors[u].add(v)\n",
    "        test_neighbors[v].add(u)\n",
    "    \n",
    "    # Степени в исходном графе\n",
    "    original_degree = {node: len(neighbors) for node, neighbors in graph.items()}\n",
    "    \n",
    "    # Бакеты\n",
    "    buckets = {\n",
    "        \"1-3\": (1, 3),\n",
    "        \"3-6\": (3, 6),\n",
    "        \"6-10\": (6, 10),\n",
    "        \"10-15\": (10, 15),\n",
    "        \"15+\": (15, float(\"inf\")),\n",
    "    }\n",
    "    \n",
    "    # Все ноды с эмбеддингами\n",
    "    all_nodes = [n for n in graph if n in model.wv]\n",
    "    node_to_idx = {n: i for i, n in enumerate(all_nodes)}\n",
    "    \n",
    "    # Матрица эмбеддингов\n",
    "    embeddings = np.array([model.wv[n] for n in all_nodes])\n",
    "    \n",
    "    # Результаты по бакетам\n",
    "    results = {bucket: {f\"recall@{k}\": [] for k in ks} | {\"mrr\": []} for bucket in buckets}\n",
    "    results[\"all\"] = {f\"recall@{k}\": [] for k in ks} | {\"mrr\": []}\n",
    "    \n",
    "    for node, true_neighbors in tqdm(test_neighbors.items(), desc=\"Evaluating\"):\n",
    "        if node not in node_to_idx:\n",
    "            continue\n",
    "        \n",
    "        node_idx = node_to_idx[node]\n",
    "        node_emb = embeddings[node_idx].reshape(1, -1)\n",
    "        \n",
    "        # Cosine similarity со всеми\n",
    "        sims = cosine_similarity(node_emb, embeddings)[0]\n",
    "        \n",
    "        # Исключаем саму ноду\n",
    "        sims[node_idx] = -1\n",
    "        \n",
    "        # Ранжируем\n",
    "        ranked_indices = np.argsort(-sims)\n",
    "        ranked_nodes = [all_nodes[i] for i in ranked_indices]\n",
    "        \n",
    "        # Метрики\n",
    "        true_set = true_neighbors & set(all_nodes)\n",
    "        if not true_set:\n",
    "            continue\n",
    "        \n",
    "        # Recall@K\n",
    "        for k in ks:\n",
    "            top_k = set(ranked_nodes[:k])\n",
    "            recall = len(top_k & true_set) / len(true_set)\n",
    "            results[\"all\"][f\"recall@{k}\"].append(recall)\n",
    "        \n",
    "        # MRR — ранг первого правильного\n",
    "        for rank, n in enumerate(ranked_nodes, 1):\n",
    "            if n in true_set:\n",
    "                results[\"all\"][\"mrr\"].append(1.0 / rank)\n",
    "                break\n",
    "        \n",
    "        # По бакетам\n",
    "        deg = original_degree.get(node, 0)\n",
    "        for bucket_name, (lo, hi) in buckets.items():\n",
    "            if lo <= deg < hi or (hi == float(\"inf\") and deg >= lo):\n",
    "                for k in ks:\n",
    "                    top_k = set(ranked_nodes[:k])\n",
    "                    recall = len(top_k & true_set) / len(true_set)\n",
    "                    results[bucket_name][f\"recall@{k}\"].append(recall)\n",
    "                for rank, n in enumerate(ranked_nodes, 1):\n",
    "                    if n in true_set:\n",
    "                        results[bucket_name][\"mrr\"].append(1.0 / rank)\n",
    "                        break\n",
    "                break\n",
    "    \n",
    "    # Агрегация\n",
    "    print(f\"\\n{'Bucket':<10} {'Count':>6} \", end=\"\")\n",
    "    for k in ks:\n",
    "        print(f\"{'R@'+str(k):>8} \", end=\"\")\n",
    "    print(f\"{'MRR':>8}\")\n",
    "    print(\"-\" * (10 + 7 + 9 * len(ks) + 9))\n",
    "    \n",
    "    for bucket_name in [\"all\"] + list(buckets.keys()):\n",
    "        data = results[bucket_name]\n",
    "        count = len(data[\"mrr\"])\n",
    "        if count == 0:\n",
    "            continue\n",
    "        print(f\"{bucket_name:<10} {count:>6} \", end=\"\")\n",
    "        for k in ks:\n",
    "            val = np.mean(data[f\"recall@{k}\"]) if data[f\"recall@{k}\"] else 0\n",
    "            print(f\"{val:>8.4f} \", end=\"\")\n",
    "        mrr = np.mean(data[\"mrr\"]) if data[\"mrr\"] else 0\n",
    "        print(f\"{mrr:>8.4f}\")\n",
    "\n",
    "evaluate(model, test_edges, graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupenv",
   "language": "python",
   "name": "jupenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
