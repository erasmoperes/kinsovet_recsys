{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-pocketbase",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pocketbase import PocketBase\n",
    "import json\n",
    "\n",
    "POCKETBASE_URL = \"http://localhost:8090\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = PocketBase(POCKETBASE_URL)\n",
    "\n",
    "similar_movies_raw = pb.collection(\"similar_movies\").get_full_list()\n",
    "similar_movies = {r.movie: set(r.recommendations) for r in similar_movies_raw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-build-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нод: 14276\n",
      "Рёбер: 35326\n",
      "Макс. рёбер: 101894950\n",
      "Плотность: 0.000347 (0.0347%)\n"
     ]
    }
   ],
   "source": [
    "# Строим граф\n",
    "graph = similar_movies\n",
    "\n",
    "# Симметризация: добавляем обратные связи\n",
    "for node, neighbors in list(graph.items()):\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor not in graph:\n",
    "            graph[neighbor] = set()\n",
    "        graph[neighbor].add(node)\n",
    "\n",
    "# Выбрасываем плохо заполненные ноды\n",
    "NODE_EDGES_THS = 0\n",
    "for node, neighbors in list(graph.items()):\n",
    "    if len(neighbors) < NODE_EDGES_THS:\n",
    "        del graph[node]\n",
    "\n",
    "# Считаем статистику\n",
    "num_nodes = len(graph)\n",
    "num_edges = sum(len(neighbors) for neighbors in graph.values()) // 2\n",
    "max_edges = num_nodes * (num_nodes - 1) // 2\n",
    "density = num_edges / max_edges if max_edges > 0 else 0\n",
    "\n",
    "print(f\"Нод: {num_nodes}\")\n",
    "print(f\"Рёбер: {num_edges}\")\n",
    "print(f\"Макс. рёбер: {max_edges}\")\n",
    "print(f\"Плотность: {density:.6f} ({density*100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего рёбер: 35326\n",
      "Train рёбер: 28261\n",
      "Test рёбер: 7065\n",
      "Доля test: 20.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def split_edges(graph, test_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Убираем test_fraction рёбер из графа.\n",
    "    Гарантируем: у каждой ноды остаётся минимум 1 ребро.\n",
    "    \"\"\"\n",
    "    all_edges = set()\n",
    "    for node, neighbors in graph.items():\n",
    "        for neighbor in neighbors:\n",
    "            edge = tuple(sorted((node, neighbor)))\n",
    "            all_edges.add(edge)\n",
    "\n",
    "    all_edges = list(all_edges)\n",
    "    random.shuffle(all_edges)\n",
    "\n",
    "    degree = defaultdict(int)\n",
    "    for u, v in all_edges:\n",
    "        degree[u] += 1\n",
    "        degree[v] += 1\n",
    "\n",
    "    current_degree = dict(degree)\n",
    "\n",
    "    test_edges = []\n",
    "    train_edges = []\n",
    "    target_test = int(len(all_edges) * test_fraction)\n",
    "\n",
    "    for u, v in all_edges:\n",
    "        if len(test_edges) < target_test and current_degree[u] > 1 and current_degree[v] > 1:\n",
    "            test_edges.append((u, v))\n",
    "            current_degree[u] -= 1\n",
    "            current_degree[v] -= 1\n",
    "        else:\n",
    "            train_edges.append((u, v))\n",
    "\n",
    "    train_graph = defaultdict(set)\n",
    "    for u, v in train_edges:\n",
    "        train_graph[u].add(v)\n",
    "        train_graph[v].add(u)\n",
    "\n",
    "    for node in graph:\n",
    "        if node not in train_graph:\n",
    "            train_graph[node] = set()\n",
    "\n",
    "    print(f\"Всего рёбер: {len(all_edges)}\")\n",
    "    print(f\"Train рёбер: {len(train_edges)}\")\n",
    "    print(f\"Test рёбер: {len(test_edges)}\")\n",
    "    print(f\"Доля test: {len(test_edges)/len(all_edges):.2%}\")\n",
    "\n",
    "    return dict(train_graph), test_edges\n",
    "\n",
    "train_graph, test_edges = split_edges(graph, test_fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-skip-gram-gpu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "SkipGram model ready\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    \"\"\"Датасет пар (target, context) из прогулок.\"\"\"\n",
    "\n",
    "    def __init__(self, walks, node_to_idx, window):\n",
    "        self.pairs = []\n",
    "        for walk in walks:\n",
    "            indices = [node_to_idx[n] for n in walk if n in node_to_idx]\n",
    "            for i, target in enumerate(indices):\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(indices), i + window + 1)\n",
    "                for j in range(start, end):\n",
    "                    if j != i:\n",
    "                        self.pairs.append((target, indices[j]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\"Skip-gram с negative sampling на GPU.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Инициализация как в Word2Vec\n",
    "        nn.init.uniform_(self.target_embeddings.weight, -0.5 / embedding_dim, 0.5 / embedding_dim)\n",
    "        nn.init.zeros_(self.context_embeddings.weight)\n",
    "\n",
    "    def forward(self, target_idx, context_idx, neg_idx):\n",
    "        # target: (batch,) -> (batch, dim)\n",
    "        target_emb = self.target_embeddings(target_idx)\n",
    "        # context: (batch,) -> (batch, dim)\n",
    "        context_emb = self.context_embeddings(context_idx)\n",
    "        # negative: (batch, num_neg) -> (batch, num_neg, dim)\n",
    "        neg_emb = self.context_embeddings(neg_idx)\n",
    "\n",
    "        # Positive score: dot product\n",
    "        pos_score = (target_emb * context_emb).sum(dim=1)\n",
    "        pos_loss = -torch.nn.functional.logsigmoid(pos_score)\n",
    "\n",
    "        # Negative score\n",
    "        neg_score = torch.bmm(neg_emb, target_emb.unsqueeze(2)).squeeze(2)\n",
    "        neg_loss = -torch.nn.functional.logsigmoid(-neg_score).sum(dim=1)\n",
    "\n",
    "        return (pos_loss + neg_loss).mean()\n",
    "\n",
    "print(\"SkipGram model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating walks...\n",
      "Generated 142760 walks\n",
      "Building dataset...\n",
      "Pairs: 25,696,800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3050e644f3d349ccbdd28cd61325a3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/6274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: avg loss = 0.4514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2bd027054645728c49a57d276d1b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/6274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: avg loss = 0.3494\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d8f786e45e4577bd96274da036a0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/6274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: avg loss = 0.3442\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fdf72ebb5d4195b8c5d3085ee4a2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/6274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: avg loss = 0.3413\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09a314fe5774e29a86e897281c2855f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/6274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: avg loss = 0.3396\n",
      "Embeddings shape: (14276, 128)\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "from pecanpy.pecanpy import SparseOTF\n",
    "\n",
    "\n",
    "def graph_to_edgelist(g, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        written = set()\n",
    "        for node, neighbors in g.items():\n",
    "            for neighbor in neighbors:\n",
    "                edge = tuple(sorted((node, neighbor)))\n",
    "                if edge not in written:\n",
    "                    f.write(f\"{edge[0]}\\t{edge[1]}\\n\")\n",
    "                    written.add(edge)\n",
    "\n",
    "\n",
    "def train_node2vec_gpu(\n",
    "    g,\n",
    "    dimensions=128,\n",
    "    window=5,\n",
    "    walk_length=20,\n",
    "    num_walks=10,\n",
    "    p=1.0,\n",
    "    q=1.0,\n",
    "    num_neg=5,\n",
    "    batch_size=4096,\n",
    "    epochs=5,\n",
    "    lr=0.005,\n",
    "    workers=4,\n",
    "):\n",
    "    # 1. Генерируем прогулки через pecanpy\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".edgelist\", delete=False) as f:\n",
    "        edgelist_path = f.name\n",
    "\n",
    "    try:\n",
    "        graph_to_edgelist(g, edgelist_path)\n",
    "        pecanpy_graph = SparseOTF(p=p, q=q, workers=workers)\n",
    "        pecanpy_graph.read_edg(edgelist_path, weighted=False, directed=False)\n",
    "        print(\"Generating walks...\")\n",
    "        walks = pecanpy_graph.simulate_walks(num_walks=num_walks, walk_length=walk_length)\n",
    "        print(f\"Generated {len(walks)} walks\")\n",
    "    finally:\n",
    "        os.unlink(edgelist_path)\n",
    "\n",
    "    # 2. Маппинг нод\n",
    "    all_nodes = sorted(g.keys())\n",
    "    node_to_idx = {n: i for i, n in enumerate(all_nodes)}\n",
    "    vocab_size = len(all_nodes)\n",
    "\n",
    "    # 3. Датасет\n",
    "    print(\"Building dataset...\")\n",
    "    dataset = SkipGramDataset(walks, node_to_idx, window)\n",
    "    print(f\"Pairs: {len(dataset):,}\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # 4. Модель\n",
    "    model = SkipGramModel(vocab_size, dimensions).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # 5. Обучение\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in pbar:\n",
    "            target, context = batch\n",
    "            target = target.to(device)\n",
    "            context = context.to(device)\n",
    "\n",
    "            # Negative sampling\n",
    "            neg = torch.randint(0, vocab_size, (target.size(0), num_neg), device=device)\n",
    "\n",
    "            loss = model(target, context, neg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix(loss=f\"{total_loss/num_batches:.4f}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: avg loss = {total_loss/num_batches:.4f}\")\n",
    "\n",
    "    # 6. Извлекаем эмбеддинги\n",
    "    embeddings = model.target_embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "    return all_nodes, node_to_idx, embeddings\n",
    "\n",
    "\n",
    "all_nodes, node_to_idx, embeddings = train_node2vec_gpu(\n",
    "    train_graph,\n",
    "    dimensions=128,\n",
    "    window=5,\n",
    "    walk_length=20,\n",
    "    num_walks=10,\n",
    "    p=1,\n",
    "    q=1,\n",
    "    epochs=5,\n",
    ")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-evaluate-gpu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786c4081dc134c4e803e1286b90f71a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bucket      Count      R@5     R@10     R@20      MRR\n",
      "-----------------------------------------------------\n",
      "all          7031   0.1444   0.2912   0.4504   0.1102\n",
      "1-3           903   0.3023   0.4219   0.5028   0.1493\n",
      "3-6          2174   0.2444   0.4210   0.5409   0.1348\n",
      "6-10         1964   0.0807   0.2679   0.4686   0.0942\n",
      "10-15        1257   0.0339   0.1463   0.3680   0.0846\n",
      "15+           733   0.0133   0.0561   0.2096   0.0763\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def evaluate_gpu(all_nodes, node_to_idx, embeddings, test_edges, graph, ks=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Evaluate на GPU: cosine similarity батчами через torch.\n",
    "    \"\"\"\n",
    "    # test-соседи\n",
    "    test_neighbors = defaultdict(set)\n",
    "    for u, v in test_edges:\n",
    "        test_neighbors[u].add(v)\n",
    "        test_neighbors[v].add(u)\n",
    "\n",
    "    original_degree = {node: len(neighbors) for node, neighbors in graph.items()}\n",
    "\n",
    "    buckets = {\n",
    "        \"1-3\": (1, 3),\n",
    "        \"3-6\": (3, 6),\n",
    "        \"6-10\": (6, 10),\n",
    "        \"10-15\": (10, 15),\n",
    "        \"15+\": (15, float(\"inf\")),\n",
    "    }\n",
    "\n",
    "    # Эмбеддинги на GPU, нормализуем для cosine similarity\n",
    "    emb_tensor = torch.tensor(embeddings, device=device, dtype=torch.float32)\n",
    "    emb_norm = F.normalize(emb_tensor, dim=1)\n",
    "\n",
    "    results = {bucket: {f\"recall@{k}\": [] for k in ks} | {\"mrr\": []} for bucket in buckets}\n",
    "    results[\"all\"] = {f\"recall@{k}\": [] for k in ks} | {\"mrr\": []}\n",
    "\n",
    "    max_k = max(ks)\n",
    "\n",
    "    # Батчевый evaluate\n",
    "    eval_nodes = [n for n in test_neighbors if n in node_to_idx]\n",
    "    eval_indices = torch.tensor([node_to_idx[n] for n in eval_nodes], device=device)\n",
    "\n",
    "    batch_size = 512\n",
    "    for start in tqdm(range(0, len(eval_nodes), batch_size), desc=\"Evaluating\"):\n",
    "        end = min(start + batch_size, len(eval_nodes))\n",
    "        batch_nodes = eval_nodes[start:end]\n",
    "        batch_idx = eval_indices[start:end]\n",
    "\n",
    "        # (batch, dim) @ (dim, vocab) -> (batch, vocab)\n",
    "        batch_emb = emb_norm[batch_idx]\n",
    "        sims = batch_emb @ emb_norm.T\n",
    "\n",
    "        # Исключаем саму ноду\n",
    "        sims[torch.arange(len(batch_idx), device=device), batch_idx] = -2.0\n",
    "\n",
    "        # Top-K на GPU\n",
    "        topk_vals, topk_indices = torch.topk(sims, max_k, dim=1)\n",
    "        topk_indices = topk_indices.cpu().numpy()\n",
    "\n",
    "        # Полный ранжинг для MRR — находим ранг первого правильного\n",
    "        ranked_all = torch.argsort(sims, dim=1, descending=True).cpu().numpy()\n",
    "\n",
    "        for i, node in enumerate(batch_nodes):\n",
    "            true_set = test_neighbors[node] & set(all_nodes)\n",
    "            if not true_set:\n",
    "                continue\n",
    "            true_idx_set = {node_to_idx[n] for n in true_set}\n",
    "\n",
    "            # Recall@K\n",
    "            for k in ks:\n",
    "                top_k_set = set(topk_indices[i, :k])\n",
    "                recall = len(top_k_set & true_idx_set) / len(true_idx_set)\n",
    "                results[\"all\"][f\"recall@{k}\"].append(recall)\n",
    "\n",
    "            # MRR\n",
    "            for rank_pos, idx in enumerate(ranked_all[i], 1):\n",
    "                if idx in true_idx_set:\n",
    "                    results[\"all\"][\"mrr\"].append(1.0 / rank_pos)\n",
    "                    break\n",
    "\n",
    "            # Бакеты\n",
    "            deg = original_degree.get(node, 0)\n",
    "            for bucket_name, (lo, hi) in buckets.items():\n",
    "                if lo <= deg < hi or (hi == float(\"inf\") and deg >= lo):\n",
    "                    for k in ks:\n",
    "                        top_k_set = set(topk_indices[i, :k])\n",
    "                        recall = len(top_k_set & true_idx_set) / len(true_idx_set)\n",
    "                        results[bucket_name][f\"recall@{k}\"].append(recall)\n",
    "                    for rank_pos, idx in enumerate(ranked_all[i], 1):\n",
    "                        if idx in true_idx_set:\n",
    "                            results[bucket_name][\"mrr\"].append(1.0 / rank_pos)\n",
    "                            break\n",
    "                    break\n",
    "\n",
    "    # Агрегация\n",
    "    print(f\"\\n{'Bucket':<10} {'Count':>6} \", end=\"\")\n",
    "    for k in ks:\n",
    "        print(f\"{'R@'+str(k):>8} \", end=\"\")\n",
    "    print(f\"{'MRR':>8}\")\n",
    "    print(\"-\" * (10 + 7 + 9 * len(ks) + 9))\n",
    "\n",
    "    for bucket_name in [\"all\"] + list(buckets.keys()):\n",
    "        data = results[bucket_name]\n",
    "        count = len(data[\"mrr\"])\n",
    "        if count == 0:\n",
    "            continue\n",
    "        print(f\"{bucket_name:<10} {count:>6} \", end=\"\")\n",
    "        for k in ks:\n",
    "            val = np.mean(data[f\"recall@{k}\"]) if data[f\"recall@{k}\"] else 0\n",
    "            print(f\"{val:>8.4f} \", end=\"\")\n",
    "        mrr = np.mean(data[\"mrr\"]) if data[\"mrr\"] else 0\n",
    "        print(f\"{mrr:>8.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = evaluate_gpu(all_nodes, node_to_idx, embeddings, test_edges, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-hyperparam-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "# Сетка гиперпараметров\n",
    "param_grid = {\n",
    "    \"dimensions\": [64, 128],\n",
    "    \"walk_length\": [20, 30],\n",
    "    \"window\": [5, 10],\n",
    "    \"p\": [0.5, 1, 2],\n",
    "    \"q\": [0.5, 1, 2],\n",
    "    \"num_walks\": [10],\n",
    "}\n",
    "\n",
    "keys = list(param_grid.keys())\n",
    "combos = list(itertools.product(*param_grid.values()))\n",
    "print(f\"Всего комбинаций: {len(combos)}\")\n",
    "\n",
    "search_results = []\n",
    "\n",
    "for i, combo in enumerate(combos):\n",
    "    params = dict(zip(keys, combo))\n",
    "    print(f\"\\n=== [{i+1}/{len(combos)}] {params} ===\")\n",
    "\n",
    "    try:\n",
    "        nodes, n2i, embs = train_node2vec_gpu(\n",
    "            train_graph,\n",
    "            dimensions=params[\"dimensions\"],\n",
    "            window=params[\"window\"],\n",
    "            walk_length=params[\"walk_length\"],\n",
    "            num_walks=params[\"num_walks\"],\n",
    "            p=params[\"p\"],\n",
    "            q=params[\"q\"],\n",
    "            epochs=5,\n",
    "        )\n",
    "\n",
    "        res = evaluate_gpu(nodes, n2i, embs, test_edges, graph)\n",
    "\n",
    "        row = dict(params)\n",
    "        for k in [5, 10, 20]:\n",
    "            row[f\"recall@{k}\"] = np.mean(res[\"all\"][f\"recall@{k}\"])\n",
    "        row[\"mrr\"] = np.mean(res[\"all\"][\"mrr\"])\n",
    "        search_results.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "        continue\n",
    "\n",
    "df_results = pd.DataFrame(search_results).sort_values(\"mrr\", ascending=False)\n",
    "print(\"\\n=== TOP RESULTS ===\")\n",
    "df_results.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
